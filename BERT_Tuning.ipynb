{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset/test.csv\n",
      "/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset/train.csv\n",
      "/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 7,613\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5462</th>\n",
       "      <td>7790</td>\n",
       "      <td>police</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Oops: Bounty hunters try to raid Phoenix polic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>10559</td>\n",
       "      <td>windstorm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@charlesadler Ian Lee's word is like 'A fart i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>7835</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reddit Will Now Quarantine Offensive Content h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>7244</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>Littleton, CO</td>\n",
       "      <td>Is your team ready for a natural disaster a vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>7918</td>\n",
       "      <td>rainstorm</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>'Three #people were #killed when a severe #rai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5262</th>\n",
       "      <td>7523</td>\n",
       "      <td>oil%20spill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Refugio oil spill may have been costlier bigge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>10745</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>khanna</td>\n",
       "      <td>Wreckage 'Conclusively Confirmed' as From MH37...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>10759</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>Southern California</td>\n",
       "      <td>Malaysian prime minister says Reunion Island w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>7392</td>\n",
       "      <td>obliterate</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Watch Sarah Palin OBLITERATE Planned Parenthoo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7426</th>\n",
       "      <td>10623</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id             keyword                  location  \\\n",
       "5462   7790              police                 Indonesia   \n",
       "7376  10559           windstorm                       NaN   \n",
       "5491   7835          quarantine                       NaN   \n",
       "5080   7244  natural%20disaster             Littleton, CO   \n",
       "5551   7918           rainstorm  United States of America   \n",
       "5262   7523         oil%20spill                       NaN   \n",
       "7512  10745            wreckage                    khanna   \n",
       "7523  10759            wreckage       Southern California   \n",
       "5180   7392          obliterate                     Texas   \n",
       "7426  10623             wounded                       NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "5462  Oops: Bounty hunters try to raid Phoenix polic...       1  \n",
       "7376  @charlesadler Ian Lee's word is like 'A fart i...       0  \n",
       "5491  Reddit Will Now Quarantine Offensive Content h...       0  \n",
       "5080  Is your team ready for a natural disaster a vi...       0  \n",
       "5551  'Three #people were #killed when a severe #rai...       1  \n",
       "5262  Refugio oil spill may have been costlier bigge...       1  \n",
       "7512  Wreckage 'Conclusively Confirmed' as From MH37...       1  \n",
       "7523  Malaysian prime minister says Reunion Island w...       1  \n",
       "5180  Watch Sarah Palin OBLITERATE Planned Parenthoo...       0  \n",
       "7426  Police Officer Wounded Suspect Dead After Exch...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "train_df = pd.read_csv('/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset/train.csv',encoding='UTF-8')\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(train_df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"         \n",
    "         u\"\\U0001F600-\\U0001F64F\"  \n",
    "         u\"\\U0001F300-\\U0001F5FF\"  \n",
    "         u\"\\U0001F680-\\U0001F6FF\"  \n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaifrehmankhan/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "\n",
    "def tweet_cleaner(text): \n",
    "   \n",
    "    try:\n",
    "        text1 = text.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        text1 = text\n",
    "    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    text1 = re.sub(r'[^\\x00-\\x7F]+',' ', text1)\n",
    "    \n",
    "    #remove emojis from tweet\n",
    "    text2 = emoji_pattern.sub(r'', text1)\n",
    "    \n",
    "    # HTML encoding\n",
    "    soup = BeautifulSoup(text2, 'lxml') \n",
    "    text5 = soup.get_text()\n",
    "    \n",
    "    # removing @ mentions\n",
    "    text6 = re.sub(pat1, '', text5)\n",
    "    \n",
    "    # Removing URLs\n",
    "    text7 = re.sub(pat2, '', text6)\n",
    "\n",
    "    # Fix misspelled words\n",
    "    text9 = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text7))\n",
    "\n",
    "    # Tokenizing ,change cases & join together to remove unneccessary white spaces\n",
    "    text9_list = tok.tokenize(text9.lower())\n",
    "    text10 = (\" \".join(text9_list)).strip()\n",
    "    \n",
    "    return text10\n",
    "\n",
    "# cleaning tweets\n",
    "train_df['text_cleaned'] = list(map(lambda x:tweet_cleaner(x),train_df['text']) )\n",
    "\n",
    "# checking out few samples\n",
    "train_df.sample(10)\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = train_df.text_cleaned.values\n",
    "labels = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      " Original:  our deeds are the reason of this # earthquake may allah forgive us all\n",
      "Tokenized:  ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', '#', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "Token IDs:  [2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  forest fire near la ronge sask . canada\n",
      "Token IDs: tensor([  101,  3224,  2543,  2379,  2474,  6902,  3351, 21871,  2243,  1012,\n",
      "         2710,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the tweet mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 70,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs:', input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,851 training samples\n",
      "2,933 training samples with real disater tweets\n",
      "  762 validation samples\n",
      "  338 validation samples with real disater tweets\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "label_temp_list = []\n",
    "for a,b,c in train_dataset:\n",
    "  label_temp_list.append(c)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} training samples with real disater tweets'.format(sum(label_temp_list)))\n",
    "\n",
    "label_temp_list = []\n",
    "for a,b,c in val_dataset:\n",
    "  label_temp_list.append(c)\n",
    "\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} validation samples with real disater tweets'.format(sum(label_temp_list)))\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Batch    40 of   215. Elapsed: 0:04:00.\n",
      "Batch    80 of   215. Elapsed: 0:07:56.\n",
      "Batch   120 of   215. Elapsed: 0:11:55.\n",
      "Batch   160 of   215. Elapsed: 0:16:02.\n",
      "Batch   200 of   215. Elapsed: 0:21:26.\n",
      "Average training loss: 0.44\n",
      "Training epoch took: 0:23:09\n",
      "Running Validation...\n",
      "Accuracy: 0.85\n",
      "Validation Loss: 0.38\n",
      "Validation took: 0:00:43\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "Batch    40 of   215. Elapsed: 0:03:52.\n",
      "Batch    80 of   215. Elapsed: 0:07:42.\n",
      "Batch   120 of   215. Elapsed: 0:11:39.\n",
      "Batch   160 of   215. Elapsed: 0:15:31.\n",
      "Batch   200 of   215. Elapsed: 0:19:18.\n",
      "Average training loss: 0.33\n",
      "Training epoch took: 0:20:46\n",
      "Running Validation...\n",
      "Accuracy: 0.86\n",
      "Validation Loss: 0.37\n",
      "Validation took: 0:00:39\n",
      "Training complete!\n",
      "Total training took 0:45:19 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def set_seed(seed_val=66):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def create_scheduler(optimizer, dataloader, epochs=2):\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    return scheduler\n",
    "\n",
    "def start_training(model, train_dataloader, valid_dataloader, optimizer, device='cpu', epochs=2):\n",
    "    scheduler = create_scheduler(optimizer, train_dataloader, epochs)\n",
    "    training_stats = train_and_evaluate(model, train_dataloader, valid_dataloader, optimizer, scheduler, epochs, device)\n",
    "    return training_stats\n",
    "\n",
    "def train_model_for_one_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'Batch {step:>5} of {len(dataloader):>5}. Elapsed: {elapsed}.')\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()    \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    return avg_train_loss, training_time\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    return avg_val_accuracy, avg_val_loss, validation_time\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, valid_dataloader, optimizer, scheduler, epochs=4, device='cpu'):\n",
    "    set_seed()\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        print(f'======== Epoch {epoch_i+1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        avg_train_loss, training_time = train_model_for_one_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "\n",
    "        print(f\"Average training loss: {avg_train_loss:.2f}\")\n",
    "        print(f\"Training epoch took: {training_time}\")\n",
    "\n",
    "        print(\"Running Validation...\")\n",
    "        avg_val_accuracy, avg_val_loss, validation_time = evaluate_model(model, valid_dataloader, device)\n",
    "\n",
    "        print(f\"Accuracy: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.2f}\")\n",
    "        print(f\"Validation took: {validation_time}\")\n",
    "\n",
    "        training_stats.append({\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        })\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Total training took {format_time(time.time()-total_t0)} (h:mm:ss)\")\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "training_stats = start_training(model, train_dataloader, validation_dataloader, optimizer, device='cpu', epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaifrehmankhan/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 7,613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "def load_and_clean_data(filepath, cleaner_function):\n",
    "    # Load the dataset into a pandas dataframe.\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Preprocess the text\n",
    "    df['text_cleaned'] = df['text'].apply(cleaner_function)\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_sentences(sentences, tokenizer, max_length=75):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,                        \n",
    "            add_special_tokens=True,      \n",
    "            max_length=max_length,           \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,   \n",
    "            return_tensors='pt',         \n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "def create_data_loader(input_ids, attention_masks, batch_size=32):\n",
    "    data = TensorDataset(input_ids, attention_masks)\n",
    "    sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "df = load_and_clean_data('/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/Dataset/train.csv', tweet_cleaner)\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "sentences = df.text_cleaned.values\n",
    "input_ids, attention_masks = encode_sentences(sentences, tokenizer)\n",
    "\n",
    "prediction_dataloader = create_data_loader(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time:  [1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def predict_sentence(sentence, model, tokenizer, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Prepare the sentence\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence,                      \n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=75,  # Pad or truncate\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attention masks\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "   \n",
    "    b_input_ids = encoded_dict['input_ids'].to(device)\n",
    "    b_input_mask = encoded_dict['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    pred_label = np.argmax(logits, axis=1)\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "# Use the function\n",
    "random_sentence = \"Last year my mother died due to flood\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Prediction time: \", predict_sentence(random_sentence, model, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/tokenizer_config.json',\n",
       " '/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/special_tokens_map.json',\n",
       " '/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/vocab.txt',\n",
       " '/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = '/Users/kaifrehmankhan/Desktop/Algorithma/Bert_Finetuning'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
